# Traffic Sign Recognition

A machine learning project that classifies traffic signs using image data. The project focuses on exploring and analyzing the Traffic Sign dataset, with experiments on additional datasets for further insights.

---

## Description

This project focuses on the development and implementation of machine learning techniques to classify images of traffic signs into their respective categories. It explores the end-to-end machine learning workflow, including data exploration, preparation, and modeling, while incorporating both traditional algorithms and modern deep learning methods.

---

## Table of Contents

- [Description](#description)
- [Project Overview](#project-overview)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Model Details](#model-details)
- [Results](#results)
- [Additional Experiments](#additional-experiments)
- [Contributing](#contributing)
- [License](#license)

---

## Project Overview:
### Part 1: Data Exploration, Preprocessing, and Naïve Bayes Classification
In Part 1, we focused on exploring the dataset, preprocessing the data, performing feature selection, and applying the Naïve Bayes classifier. Below are the key findings:

#### Data Visualization and Exploration:
- We visualized the class distribution using a pie chart and examined the pixel intensity distribution, revealing that most pixels range from 0 to 50, indicating darker regions.
#### Image Enhancement:
- Applied Histogram Equalization and Gamma Correction to improve image contrast and brightness, enhancing the greyscale images for better feature extraction.
#### Data Preprocessing:
- Normalized pixel values by dividing by 255.
- Handled missing values (none found).
- Outliers were identified using DBSCAN, which detected 3,680 outliers.
#### Feature Selection:
- Used SelectKBest to retain the top 5, 10, and 20 features for each class, forming datasets with progressively fewer features for analysis.
#### Model Evaluation:
- Evaluated three Naïve Bayes classifiers (Gaussian, Multinomial, and Complement) using K-Fold Cross Validation. The models performed best with the top 5 features, with the Multinomial Naïve Bayes achieving an accuracy of 60.8%.

#### Confusion Matrix and ROC Curve:
- Generated confusion matrices and ROC curves for each model, showing that feature selection enhanced the model's performance, with the highest ROC AUC achieved using the top 5 features.

### Part 2: K-Means Clustering
In Part 2, we applied K-Means clustering to our dataset and evaluated its performance. Additionally, we explored various clustering algorithms (both hard and soft clustering), compared their effectiveness, and investigated methods for determining the optimal number of clusters.

#### Data Preprocessing:
- Principal Component Analysis (PCA) was used to reduce dimensionality, with 5 components chosen for the reduction.

#### K-Means Clustering:
- We applied K-Means to the complete dataset and the dataset with top 5 features, achieving accuracies of 35.72% and 53.08%, respectively.
- Visualization of clusters was performed using PCA, scatter plots, histograms, and violin plots.

#### Clustering Algorithms Explored:
- Hard Clustering: DBSCAN, BIRCH, and Agglomerative Clustering.
- Soft Clustering: Gaussian Mixture Model (GMM), Fuzzy C-Means, and Self-Organized Maps (SOM).
- PCA significantly improved clustering performance across all algorithms.
#### Optimal Number of Clusters:
- We evaluated the Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index to determine the best number of clusters, with the top 5 features dataset performing best.

#### Clustering vs. Bayesian Classification:
- K-Means clustering accuracy was compared with Bayesian models (Gaussian Naive Bayes and Multinomial Naive Bayes), showing that Naive Bayes models outperformed K-Means on the complete and top 5 features datasets.

### Part 3: Decision Tree Performance: Cross-Validation and Train-Test Classifier
In Part 3, we focused on training a Decision Tree classifier using different data splits and evaluated its performance. We also experimented with various decision tree parameters and compared it with other models.

Preprocessing:

Image Enhancement: Applied techniques like histogram equalization, gamma correction, and normalization (dividing values by 255).
Outlier Detection: Outliers were removed using DBScan.
Oversampling: Used to balance class distribution in the training dataset.
Decision Tree Training:

We trained the Decision Tree on different train-test splits (70:30, 60:40, 80:20) and evaluated accuracy.
The model achieved a maximum accuracy of 94.85% with the 60:40 split.
Cross-validation: Average accuracy of 92.57% from 10-fold cross-validation.
Evaluation Metrics:

Metrics used: Accuracy, Precision, Recall, F1 Score, Sensitivity, Specificity, MAE, TP, TN, FP, FN rates.
Model Findings:

Overfitting: A 100% accuracy on the training set indicated potential overfitting, with test accuracy dropping significantly (from ~94% to 71%).
The model's performance varied with different splits, showing that larger test sets reduced overfitting.
Parameter Tuning:

Experimented with parameters like max_depth, min_samples_leaf, and criterion to control tree complexity.
The best model was identified using cross-validation with optimal parameters.
Comparison with Other Models:

Random Forest and Extra Trees classifiers were tested alongside Decision Trees.
Random Forest achieved the highest accuracy (99.36%) and cross-validation (98.31%), followed by Extra Trees (99.08%, 97.74%).
Conclusion:

Decision Trees showed overfitting, but Random Forests outperformed all models.
The decision tree's performance improved with different train-test splits, and further experiments with tree depth and other parameters helped reduce overfitting.








#### Part 3- Supervised Learning:
* Implemented and compared various machine learning algorithms, including:
1. Naïve Bayes
2. Linear Regression
3. K-Nearest Neighbors (KNN)
4. Decision Trees
5. Addressed challenges such as overfitting and generalization using cross-validation and regularization.

#### Advanced Machine Learning Techniques:
* Designed and trained Artificial Neural Networks (ANNs) for enhanced accuracy.
* Leveraged Convolutional Neural Networks (CNNs) to process and classify image data efficiently.



## Features

- Classification of traffic signs into multiple categories.
- Data preprocessing, including normalization and augmentation.
- Model training using a convolutional neural network (CNN).
- Comparison of performance with other models and datasets.

---

## Dataset

- **Primary Dataset**: Traffic Sign Dataset
  - Contains images of traffic signs with corresponding labels.
  - Preprocessed to resize images and normalize pixel values.
  
- **Additional Datasets**:
  - GTSRB (German Traffic Sign Recognition Benchmark)
  - CIFAR-10 for experimentation with general object classification.

---

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/traffic-sign-recognition.git
